{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.LongTensor([[1,2,4,5],[4,3,2,9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from packages.functions import num_to_var\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the output features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*output \\\\\n",
    "            attn = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            output = \\tanh(w * (attn * context) + b * output)\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the output\n",
    "    Inputs: output, context\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n",
    "        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    Examples::\n",
    "         >>> attention = seq2seq.models.Attention(256)\n",
    "         >>> context = Variable(torch.randn(5, 3, 256))\n",
    "         >>> output = Variable(torch.randn(5, 5, 256))\n",
    "         >>> output, attn = attention(output, context)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_out = nn.Linear(dim*2, dim)\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, output, context):\n",
    "        batch_size = output.size(0)\n",
    "        hidden_size = output.size(2)\n",
    "        input_size = context.size(1)\n",
    "        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n",
    "        attn = torch.bmm(output, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn = F.softmax(attn.view(-1, input_size)).view(batch_size, -1, input_size)\n",
    "\n",
    "        # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n",
    "        mix = torch.bmm(attn, context)\n",
    "\n",
    "        # concat -> (batch, out_len, 2*dim)\n",
    "        combined = torch.cat((mix, output), dim=2)\n",
    "        # output -> (batch, out_len, dim)\n",
    "        output = F.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n",
    "\n",
    "        return output, attn\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args,config):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #Arguments\n",
    "        \n",
    "        \n",
    "        #Model Requirements\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        if initial_embedding is not None:\n",
    "                    #TODO initialize embeddings with the embeddings received also decide trainable or not trainable\n",
    "        else:\n",
    "            #TODO randomly initialize embeddings\n",
    "        \n",
    "        self.projection = nn.Linear(hidden_size, len_vocab)\n",
    "        self.cell_encoder = nn.GRU(input_size=embed_size,\n",
    "            hidden_size=hidden_size, batch_first=True,\n",
    "            bidirectional=True)\n",
    "        if config.config_dir[\"same_cell\"] == True:\n",
    "            self.cell_query = self.cell_encoder\n",
    "        else:\n",
    "            self.cell_query = nn.GRU(input_size=embed_size,\n",
    "                hidden_size=hidden_size, batch_first=True,\n",
    "                bidirectional=True)\n",
    "            \n",
    "        self.cell_decoder = nn.LSTM(input_size=embed_size,\n",
    "            hidden_size=hidden_size, batch_first=True)\n",
    "        self.DistractionDecoder = Decoder(TODO, parameters)\n",
    "        if (config.config_dir[\"is_bidir\"]):\n",
    "        \ths = 2*hidden_size\n",
    "        else:\n",
    "        \ths = hidden_size\n",
    "        self.attention = Attention(hidden_size*2, hidden_size)\n",
    "        if config.config_dir[\"distraction_cell\"] == \"LSTM_soft\":\n",
    "        \tdistract_cell = DistractionLSTMCell_soft(hs, state_is_tuple = True)\n",
    "        elif config.config_dir[\"distraction_cell\"] == \"LSTM_hard\":\n",
    "        \tdistract_cell = DistractionLSTMCell_hard(hs, state_is_tuple=True)\n",
    "        elif config.config_dir[\"distraction_cell\"] == \"LSTM_sub\":\n",
    "        \tdistract_cell = DistractionLSTMCell_subtract(hs, state_is_tuple=True)\n",
    "        elif config.config_dir[\"distraction_cell\"] == \"GRU_hard\":\n",
    "        \tdistract_cell = DistractionGRUCell_hard(hs) \n",
    "        elif config.config_dir[\"distraction_cell\"] == \"GRU_soft\":\n",
    "        \tdistract_cell = DistractionGRUCell_soft(hs)\n",
    "        elif config.config_dir[\"distraction_decoder_start_cell\"] == \"GRU_sub\":\n",
    "        \tdistract_cell = DistractionGRUCell_subtract(hs)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, encoder_inputs,query_inputs, target, batch, vocab, train=True):\n",
    "        encoder_embedded_inputs = self.embed(encode_inputs)\n",
    "        query_embedded_inputs = self.embed(query_inputs)\n",
    "        encoder_outputs, encoder_state = self.cell_encoder(encoder_embedded_inputs)\n",
    "        query_outputs, query_state = self.cell_query(query_embedded_inputs)\n",
    "        \n",
    "        d = encoder_outputs[-1]\n",
    "        ey =  \n",
    "        for i, inp in enumerate(target):\n",
    "            _, s = self.cell_decoder(s, )\n",
    "        \n",
    "        # TODO Do Attention Calculation as required code examples from BIMPM or Gettothepoint\n",
    "        \n",
    "        \n",
    "        # TODO Decoder and inside Decoder - Diversity\n",
    "        \n",
    "        return out_list, cov_loss\n",
    "\n",
    "    def mask(self, matrix):\n",
    "        # for obtaining a FloatTensor Variable mask out of a LongTensor Variable\n",
    "        out = matrix==0\n",
    "        return Variable(out.float().data)\n",
    "\n",
    "    def to_cuda(self, tensor):\n",
    "        # turns to cuda\n",
    "        if torch.cuda.is_available():\n",
    "            return tensor.cuda()\n",
    "        else:\n",
    "            return tensor\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
